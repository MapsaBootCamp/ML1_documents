**Scrapy**

Web scraping  به فرآیند استخراج داده از وبسایت‌ها به صورت خودکار گفته می‌شود. به طور سنتی دو ابزار HTTrack و Wget برای این کار استفاده می‌شدند؛ ولی این‌ها تمام یک سایت را دانلود می‌کنند و ایراد اصلی آن‌ها این است که نمی‌توانند تنها یک قسمت از داده‌ها را به صورت مشخص جدا کنند. همچنین امکان خروجی گرفتن از داده‌ها و ساختاربندی آن‌ها هم فراهم نیست.

راه بهتر برای استخراج دادن ساختن یک ابزار با استفاده از فریم‌ورک scrapy است.  اسکرپی رایج‌ترین فریم‌ورک موجود برای ساختن spider است . روش کار اسکرپی به صورت زیر است: 

در مرحله‌ی اول Engine (که نقش هماهنگ‌کننده دارد) از Spider درخواست (Request) را دریافت می‌کند و آن را به Scheduler می‌رساند. وظیفه‌ی Scheduler بررسی ترتیب درخواست‌هاست. در صورتی که درخواست دیگری قبلا فرستاده نشده باشد، Scheduler همان درخواست دریافتی را مجددا به Engine می‌فرستد. Engine درخواست را به سمت Downloader می‌فرستد تا از آن پاسخی (Response) دریافت کند. سپس پاسخ دریافتی را به Spider می‌دهد تا پردازش شود. در نهایت، Engine پیام را به سمت ITEM PIPELINES می‌فرستد تا قسمتی‌های مشخصی از داده را که دستور جدا کردنش را دارد خارج کند.

![](Aspose.Words.a534b463-8dc5-4d3f-a48e-9e6700793be8.001.png)





یک کد نمونه‌ی اسکرپی به شکل زیر است:

**import** **scrapy**


**class** **QuotesSpider**(scrapy.Spider):

`    `name = 'quotes'

`    `start\_urls = [

`        `'http://quotes.toscrape.com/tag/humor/',

`    `]

`    `**def** parse(self, response):

`        `**for** quote **in** response.css('div.quote'):

`            `**yield** {

`                `'author': quote.xpath('span/small/text()').get(),

`                `'text': quote.css('span.text::text').get(),

`            `}

`        `next\_page = response.css('li.next a::attr("href")').get()

`        `**if** next\_page **is** **not** **None**:

`            `**yield** response.follow(next\_page, self.parse)

در این‌جا اسکرپی به آدرسی که در start\_urls مشخص شده ریکوئست می‌فرستد و پاسخ‌ها را به عنوان argument برای متد parse می‌فرستد. در parse با استفاده از selectorهای CSS المان‌ها را بررسی می‌کند و در یک دیکشنری پایتون yield می‌کند. در قسمت بعدی  اسکرپی صفحات بعدی را نیز بررسی می‌کند و مجددا ریکوئست می‌فرستد.

مزیت اصلی اسکرپی این است که می‌تواند ریکوئست‌ها را همزمان و به صورت برنامه‌ریزی‌شده بفرستد. به عبارت دیگر، لازم نیست صبر کند تا پاسخ یک ریکوئست بیاید و بعد ریکوئست بعدی را ارسال کند. بنابراین، اگر یکی از ریکوئست‌ها با مشکل مواجه شود و یا ارور دهد، برنامه از کار نمی‌افتد و بدین ترتیب، سرعت crawl کردن صفحات بسیار بالا می‌رود.
